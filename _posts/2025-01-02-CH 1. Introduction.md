---
title: >-
    [단단한 강화학습] CH 1. Introduction
author: csm
date: 2025-01-02 15:33:00 +0900
categories: [AI, ⟪Reinforcement Learning⟫]
tags: [book]
description: >-
    ⟪Reinforcement Learning⟫, Richard S.Sutton and Andrew G.Barto
math: true
---

- 이상적인 학습 환경 가정, 학습 방법이 갖는 효과 분석
- 학습의 문제를 해결하는 기계 설계, 기계의 성능 분석

## 1.1 Reinforcement Learning
---
- Reinforcement learning: 주어진 상황에서 어떠한 행동을 취할 지 학습하는 것
- 학습자(agent) ⟶ 시행착오(행동) ⟶ 보상 ⟶ 학습(최대의 보상을 갖는 행동 찾기)
    + 지연된 보상 상황 고려
- 지도학습과의 차이: 강화학습에는 지침(label) X
- 비지도학습과의 차이: 강화학습은 숨겨진 구조 찾기 X
- 강화학습의 핵심
    - 활용(exploitation): 이미 경험한 행동 이용
    - 탐험(exploration): 미래에 더 좋은 행동을 선택하기 위함
    +  ⟶ 탐험과 활용은 최대보상을 향한다.
    +  ⟶ 탐험과 활용의 딜레마
    - 불확실한 주변 환경과 <u>상호작용</u>하는 <u>목표 지향적인 학습자</u>에 대한 모든 문제를 분명하게 고려(학습자는 불확실한 환경에 있지만, 학습을 수행해야 한다는 것을 인지한 상태)
- '차원의 저주' 문제 해결

## 1.2 Examples
---
- 예제들의 공통 요소: 학습자, 불확실한 요소를 포함하는 주변 환경, 상호작용, 목표
- 학습자: 경험 ⟶ 행동의 능력 ↑

## 1.3 Elements of Reinforcement Learning
---
- 학습자, 주변 환경 외 나머지 주요한 구성 요소
    - 정책(policy): 특정 시점에 학습자가 취하는 행동 정의
    - 보상 신호(reward signal): 강화학습이 성취해야할 목표 정의. 순간의 상태에 내재된 고유의 장점
    - 가치 함수(value function): 장기적 관점으로 평가한 상태의 장점(보상에 대한 예측) ⟶ 앞으로 주요하게 다뤄질 가치 추정
    - 모델(model): 환경의 변화를 모사. 계획(planning, 가능성만을 고려하여 일련의 행동 결정)을 위해 사용

## 1.4 Limitations and Scope
---
- 상태(state): 특정 시각에 환경이 어떤 모습을 하고 있는지에 대한 정보를 학습자에게 전달하는 신호  
    정책과 가치 함수의 입력, 혹은 모델의 입력과 출력  
    학습자가 사용할 수 있는 환경에 대한 모든 정보
- 강화학습 방법
    - 가치 함수를 추정하기 위한 것. 환경과 상호작용하며 학습습
    - 유전자 알고리즘, 유전자 프로그래밍, 모의 담금질 같은 최적화 방법
        - 다수의 정적 정책(statice policy) 적용: 오랜시간 불연속적 상호작용
        - 진화적(evolutionary) 방법: 진화를 통해 생산된 유기체는 학습 없이도 노련한 행동

## 1.5 An Extended Example: Tic-Tac-Toe
---
- 미니맥스(minimax, 게임 이론의 전통적 방법)의 경우: 상대방이 특정한 방법으로 게임을 한다는 가정 필요
- 동적 프로그래밍(dynamic programming, 순차적 결정 문제에 대한 전통적 최적화 방법)의 경우: 상황에 따라 특정 선택을 할 확률 등 상대방에 대한 완벽한 정보 필요
- 진화적 방법의 경우: 정책(규칙)을 평가하고 더 향상된 정책을 찾아감
- 가치 함수 사용(강화학습)의 경우: 
    1. '상태'에 숫자 부여 ⟶ 숫자 표 생성 ⟶ 전체 표는 학습된 가치 함수  
    2. 여러 번의 게임 진행: 탐욕적 선택, 탐험적 선택  
        ⟶ 탐욕적 선택 이후 결정될 상태의 가치를 선택 이전의 상태에 보강(backup)  
        ⟹ 가치 함수를 이용하는 방법은 진화적 방법과 달리 개별적인 상태들을 평가
    - 갱신 규칙: 시간차 학습   
    - $$V(S_{t})$$로 표현되는 $$S_{t}$$ 추정값의 갱신: $$V(S_{t})  \leftarrow  V(S_{t}) + \alpha \left [V(S_{t+1}) - V(S_{t})  \right ]$$  
        $$S_{t}$$: 탐욕적 선택 이전의 상태, $$S_{t+1}$$: 탐욕적 선택 이후의 상태, $$\alpha$$: 시간 간격 파라미터(step-size parameter) 
    - $$\alpha$$ ↓ ⟶ 표의 확률값이 참값으로 수렴  
    - 강화학습의 핵심 특성:  
        1__ 주변 환경과 상호작용하며 학습하는 것 강조  
        2__ 확실한 목표 존재, 행동의 지연된 효과 고려하는 계획 또는 예지 필요
    
## 1.6 Summary
---
- 강화학습은 장기적인 목표 지향적, 환경과 상호작용하며 학습, 그 과정에서 발생하는 계산과 결정 이해
- 강화학습은 마르코프 결정 과정의 형식적 틀 사용
- 강화학습은 가치 함수를 사용한다는 점에서 진화적 방법과 구별

## 1.7 Early History of Reinforcement Learning
---
- 초기역사
    1. 시행착오로부터터 학습
    2. 가치 함수와 동적 프로그래밍을 이용하는 최적 제어의 문제와 그 해결책
    3. 시간차 방법

### 가치 함수와 동적 프로그래밍을 이용하는 최적 제어의 문제와 그 해결책
- 최적 이득 함수(optimal return function) ⟶ 동적 프로그래밍(차원의 저주 문제)
- 마르코프 결정 과정(MDP, 확률론적 최적 제어의 문제) 해결 방식
    - 이산 확률론적(discrete stochastic)
    - 정책 반복(policy iteration)
- 최적 제어: 어떤 동역학 싯트메의 시간에 따른 결과를 측정하고 그 측정값을 최대 또는 최소화
    - 동적 프로그래밍: 시간의 역순으로 진행하는 계산
    - 학습: 시간 순으로 진행하는 과정
    - 동적 프로그래밍과 학습의 연결: ex. 신경동역학 프로그래밍(neurodynamic programming) = 근사적 동적 프로그래밍(approximate dynamic programming)

### 시행착오로부터 학습
- '더듬기와 실험(groping and experiment)'에 의한 학습  
    효과의 법칙(Low of Effect): 행동 선택의 경향을 강화하는 사건의 효과를 나타냄
- 전기 기계식 학습 장치 [cyberneticzoo.com](https://cyberneticzoo.com/)
- 논문 ｢인공지능을 향한 단계들(Step Toward Artificial Intelligence)｣: '강화', '강화학습'이 시행착오 학습의 공학적 사용을 설명하기 위해 사용
- STeLLA: 내적 독백(internal monologue), 누출 과정(leakback process)
- MENACE / GLEE-BOXES
- 선택적 부트스트랩 적응(selective bootstrap adaptation): learning with a critic
- 학습로봇(learning automata): 다중 선택(K-armed bandit)
    - 확률론적 학습로봇(stochastic learning automata): Alopex 알고리즘의 영향
- 게임이론과 경제학 모델에 적용
- 분류 시스템(classifier system): 양동이 집합(bucket-brigade) 알고리즘, 유전자(genetic) 알고리즘
- 헤리 클로프: 지도학습에 초점을 맞추면 적응하는 행동의 본질적 측면을 잃어버린다.

### 시간차 방법
- 특정 값을 시간에 따라 연속적으로 추정하고 연속한 두 추정값 사이의 차이로부터 학습 방법을 도출
- 2차 강화자(secondary reinforcer) / 2차 강화 이론
- 지역 강화(local reinforcement): 전체 학습 시스템의 하위 요소들이 서로 강화  
    일반화된 강화학습(generalized reingorcement): 모든 구성요소가 모든 입력을 강화의 측면에서 인식
- 시간차 학습에 기반한 고전적 조건화(conditioning)

### 이후
- 행동자-비평자 구조(actor-critic architecture): 시간차 학습과 시행착오 학습의 결합
- Q 학습: 시간차와 최적제어 함께 도입
- 폴 웨어보스: 시행착오 학습 + 동적 프로그래밍
- 백개먼 프로그램 / TD-Gammon